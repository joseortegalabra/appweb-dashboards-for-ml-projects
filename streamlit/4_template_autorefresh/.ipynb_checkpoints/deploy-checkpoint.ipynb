{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea04d11-b156-4dff-ac80-08ab3e80c79b",
   "metadata": {},
   "source": [
    "# Deploy streamlit app into a cloud run (gcp)\n",
    "\n",
    "**Following the ideas of:**\n",
    "- deploy streamlit app in cloud run: https://medium.com/@faizififita1/how-to-deploy-your-streamlit-web-app-to-google-cloud-run-ba776487c5fe\n",
    "- deploy streamlit app into google app engine: https://dev.to/whitphx/how-to-deploy-streamlit-apps-to-google-app-engine-407o\n",
    "- deploy a flask app into a cloud run: my previous codes\n",
    "    - Create Dockerfile (an example Dockerfile can be found at: https://firebase.google.com/docs/hosting/cloud-run?hl=es-419#python)\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "- Create a sript with codes\n",
    "- Create a .env file with environment variables that you don't want to expose\n",
    "- Define the parameters to deploy\n",
    "- Run codes that do diferents steps to deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc770c07-ce2a-492f-97ac-bd9fb8aaa545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c6054b-c6e1-4a49-b788-156a92e3a057",
   "metadata": {},
   "source": [
    "## I) INTRODUCTION\n",
    "\n",
    "These codes could be run in the Google SDK console, as well as run on a notebook.\n",
    "For a data scientist who is not specialized in devops practices, using a notebook is more intuitive to use\n",
    "\n",
    "#### Previous steps\n",
    "- Stop in the root folder of the application (this notebook is created in the root)\n",
    "- Have scripts created to contain and upload to cloud run\n",
    "\n",
    "#### Important information\n",
    "**To run a console command in a Jupyter notebook and the python variables stored in the notebook can also be passed to the command, you must use the peso sign ($) and not use the assignment command (=)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c5998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646304fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef96bd-3a6c-48df-8078-6efaf5b6c09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bf9ae26",
   "metadata": {},
   "source": [
    "## II) INITIALIZE READ .ENV WITH ENV VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338a3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv # package used in jupyter notebook to read the variables in file .env\n",
    "\n",
    "\"\"\" get env variable from .env \"\"\"\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\"\"\" Read env variables and save it as python variable \"\"\"\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_GCP\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e91a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42cc73c-8063-4e9f-96c7-ae5343c55c53",
   "metadata": {},
   "source": [
    "## III) DEFINE PARAMETERS TO DEPLOY APP INTO A CLOUD RUN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4bd3e5-8975-45c0-abbc-a5eea58acfd0",
   "metadata": {},
   "source": [
    "### Step 0: Connect to GCP project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9d1563e-5625-4963-952d-4c6468d261c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db232b7-2239-4ccc-b8f4-7c313b09cd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d690650-e360-4d14-9c69-e792e32eb0f1",
   "metadata": {},
   "source": [
    "### Step 1: Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fe41fe-0fdc-4999-b2f8-7e225cfdb5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARARAMETERS\n",
    "\n",
    "# general gcp\n",
    "REGION = 'us-east1'\n",
    "\n",
    "# name of the repo in artifact registry where will be saved docker images\n",
    "NAME_REPO = 'repo-app-web-4-template-autorefresh-streamlit'\n",
    "FORMAT_REPO = 'docker'\n",
    "DESCRIPTION_REPO = \"repo web app 4 template autorefresh page in streamlit - with a package https://github.com/kmcgrady/streamlit-autorefresh\"\n",
    "\n",
    "# name of the docker image saved in docker repo in artifact registry\n",
    "NAME_IMAGE = '4-template-autofresh-stremalit'\n",
    "\n",
    "# name of cloud run where the app web will be located\n",
    "NAME_CLOUD_RUN = 'app-web-4-template-autorefesh-streamlit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839eb1d7-f6ba-4d98-ba9b-2490e1cc66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0591b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4eda8-348f-439f-9413-adba04c635ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e1c63e",
   "metadata": {},
   "source": [
    "# IV) Upload a docker image with the codes of the app in Artifact Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179f0053-ca26-44cf-ac37-de8928fe4bc9",
   "metadata": {},
   "source": [
    "- Artifact registry is the replacement for container registry and recommended by google. The only difference is that the image is saved in this new service and you need to run another command\n",
    "\n",
    "- **Additionally, every time a new image is uploaded to the artifact registry (rerun the corresponding gcloud command), it receives the latest tag and is the one used to create/update the created cloud run**\n",
    "\n",
    "- Uploading the image to the artifact Registry requires more steps than uploading it to the container registry\n",
    "\n",
    "- Cloud build integration documentation with Artifact Registry: https://cloud.google.com/artifact-registry/docs/configure-cloud-build?hl=es-419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072af20-fc4b-4bd0-87f3-eac65495a710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccab59b-8d26-4e56-ac6f-61d71014890d",
   "metadata": {},
   "source": [
    "### Step 1. Create repository in artifact registry (if it does not exist)\n",
    "- Unlike container registry which was automatic, in artifact registry you have to create it. **If the repo already exists the gcloud command return an error but doesn't stop de execution of the notebook**\n",
    "\n",
    "- A repo is created which can have multiple images and each one have different versions\n",
    "\n",
    "- Documentation: create repo in artifact registry: https://cloud.google.com/artifact-registry/docs/repositories/create-repos#gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceae1f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n"
     ]
    }
   ],
   "source": [
    "# create repo artifact registry\n",
    "! gcloud artifacts repositories create $NAME_REPO \\\n",
    "--repository-format $FORMAT_REPO \\\n",
    "--location $REGION \\\n",
    "--description \"$DESCRIPTION_REPO\" \\\n",
    "--async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab2428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c040f26c-e1a6-400d-a358-5502c822f932",
   "metadata": {},
   "source": [
    "### Step 2: Set up a Docker build\n",
    "\n",
    "It is necessary to create a **yaml** with the configuration to build the docker image in Artifact Registry.\n",
    "\n",
    "It has the following form\n",
    "\n",
    "<code>\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "   args: [ 'build', '-t', '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}', '.' ]\n",
    "images:\n",
    "- '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}'\n",
    "<code>\n",
    "    \n",
    "\n",
    "**This is a GENERIC FILE that can be recycled because it is parameterized to work with any docker repo in the artifact registry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dcace8-1a0f-4833-89ab-544bfc2aa7c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f13b0ae",
   "metadata": {},
   "source": [
    "--> Running the following line of code creates a yaml file with the desired configuration.\r\n",
    "\r\n",
    "Documentation: https://stackabuse.com/reading-and-writing-yaml-to-a-file-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98471485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# create a python diccionary with the content of the yaml cloudbuild generic\n",
    "\n",
    "dict_python_yaml_cloudbuild = {'steps': [{'name': 'gcr.io/cloud-builders/docker',\n",
    "   'args': ['build', '-t', '${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}', '.']}],\n",
    " 'images': ['${_LOCATION}-docker.pkg.dev/$PROJECT_ID/${_REPOSITORY}/${_IMAGE}']}\n",
    "\n",
    "# save dicctionary in yaml format\n",
    "with open(r'cloudbuild.yaml', 'w') as file:\n",
    "    documents = yaml.dump(dict_python_yaml_cloudbuild, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e475e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b267e53",
   "metadata": {},
   "source": [
    "### Step 3: Create Dockerfile\n",
    "Like the previous step, in this case you need to create the Dockerfile to be able to upload the image to the Artifact Registry. Typically, the dockerfile is created manually.\r\n",
    "\r\n",
    "In this example, the content of the dockerfile is defined within a parameterized string, which allows it to be a transversal file and applicable to any deploy in a cloud run\r\n",
    "\r\n",
    "Generally, most of the time, **the Dockerfile does not need to be modified unless you want to change the python vers**ion. So by running the following code you obtain the Dockerfile of the web app\r\n",
    "\r\n",
    "https://prnt.sc/uwBOFChK8QU8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0e2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A STRING THAT REPRESENT THE DOCKER FILE\n",
    "\n",
    "#old version with requirements file inside the script\n",
    "# string_dockerfile = '''\n",
    "# FROM python:3.10\n",
    "# EXPOSE 8080\n",
    "# WORKDIR /app\n",
    "# COPY . ./\n",
    "# RUN pip install streamlit gunicorn\n",
    "# ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "# '''\n",
    "\n",
    "\n",
    "string_dockerfile = '''\n",
    "FROM python:3.10\n",
    "EXPOSE 8080\n",
    "WORKDIR /app\n",
    "COPY . ./\n",
    "RUN pip install -r requirements.txt\n",
    "ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc57cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar dockerfile\n",
    "with open('Dockerfile', 'w') as file:\n",
    "    file.write(string_dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b43e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15903a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba335ac5",
   "metadata": {},
   "source": [
    "### Step 4: Containerize (docker image) web app codes using cloud build and upload them to artifact registry\r\n",
    "- In this step, a docker image is created with the necessary codes for the web app and then this image is uploaded to Artifact Registry (using as a base the \"cloudbuild.yaml\" file that calls the \"Dockerfile\", created in the steps previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f993074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 9 file(s) totalling 71.4 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1706280565.768663-3b2dba8108bd46918e895b93933d830a.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/cmpc-innovation-cd4ml-test/locations/global/builds/a05b31fc-0b27-4809-ae3c-b4b19757eef1].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/a05b31fc-0b27-4809-ae3c-b4b19757eef1?project=724348686027 ].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting build \"a05b31fc-0b27-4809-ae3c-b4b19757eef1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1706280565.768663-3b2dba8108bd46918e895b93933d830a.tgz#1706280557528201\n",
      "Copying gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1706280565.768663-3b2dba8108bd46918e895b93933d830a.tgz#1706280557528201...\n",
      "/ [0 files][    0.0 B/ 20.1 KiB]                                                \n",
      "/ [1 files][ 20.1 KiB/ 20.1 KiB]                                                \n",
      "\n",
      "Operation completed over 1 objects/20.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  82.43kB\n",
      "\n",
      "\n",
      "Step 1/6 : FROM python:3.10\n",
      "3.10: Pulling from library/python\n",
      "1b13d4e1a46e: Already exists\n",
      "1c74526957fc: Already exists\n",
      "30d855997954: Pulling fs layer\n",
      "ad5739181616: Pulling fs layer\n",
      "75e2b45cbee5: Pulling fs layer\n",
      "37a8a17eedd2: Pulling fs layer\n",
      "2697e3bb938f: Pulling fs layer\n",
      "1db75c2e70ec: Pulling fs layer\n",
      "37a8a17eedd2: Waiting\n",
      "2697e3bb938f: Waiting\n",
      "1db75c2e70ec: Waiting\n",
      "75e2b45cbee5: Verifying Checksum\n",
      "75e2b45cbee5: Download complete\n",
      "30d855997954: Verifying Checksum\n",
      "30d855997954: Download complete\n",
      "37a8a17eedd2: Verifying Checksum\n",
      "37a8a17eedd2: Download complete\n",
      "2697e3bb938f: Verifying Checksum\n",
      "2697e3bb938f: Download complete\n",
      "1db75c2e70ec: Verifying Checksum\n",
      "1db75c2e70ec: Download complete\n",
      "ad5739181616: Verifying Checksum\n",
      "ad5739181616: Download complete\n",
      "30d855997954: Pull complete\n",
      "ad5739181616: Pull complete\n",
      "75e2b45cbee5: Pull complete\n",
      "37a8a17eedd2: Pull complete\n",
      "2697e3bb938f: Pull complete\n",
      "1db75c2e70ec: Pull complete\n",
      "Digest: sha256:11edd7f5ed19b19adfc91d3384970da0787d53cf290a54db5f96b4dab645da6d\n",
      "Status: Downloaded newer image for python:3.10\n",
      " ---> e399a6a8821d\n",
      "Step 2/6 : EXPOSE 8080\n",
      " ---> Running in 1d315204dea2\n",
      "Removing intermediate container 1d315204dea2\n",
      " ---> a4eb5c443d12\n",
      "Step 3/6 : WORKDIR /app\n",
      " ---> Running in 7e1efbc3d143\n",
      "Removing intermediate container 7e1efbc3d143\n",
      " ---> ea14a83b85f0\n",
      "Step 4/6 : COPY . ./\n",
      " ---> 685fd6a30668\n",
      "Step 5/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in eae6be3c4c3d\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.30.0-py2.py3-none-any.whl (8.4 MB)\n",
      "     ???????????????????????????????????????? 8.4/8.4 MB 25.5 MB/s eta 0:00:00\n",
      "Collecting streamlit-autorefresh\n",
      "  Downloading streamlit_autorefresh-1.0.1-py3-none-any.whl (700 kB)\n",
      "     ?????????????????????????????????????? 700.8/700.8 kB 38.4 MB/s eta 0:00:00\n",
      "Collecting pytz\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "     ?????????????????????????????????????? 502.5/502.5 kB 45.8 MB/s eta 0:00:00\n",
      "Collecting gunicorn\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "     ???????????????????????????????????????? 80.2/80.2 kB 14.6 MB/s eta 0:00:00\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "     ???????????????????????????????????????? 15.6/15.6 MB 66.0 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "     ???????????????????????????????????????? 13.0/13.0 MB 76.2 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "     ???????????????????????????????????????? 18.2/18.2 MB 66.1 MB/s eta 0:00:00\n",
      "Collecting watchdog>=2.1.5\n",
      "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "     ???????????????????????????????????????? 82.1/82.1 kB 14.5 MB/s eta 0:00:00\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting protobuf<5,>=3.20\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     ?????????????????????????????????????? 294.6/294.6 kB 38.3 MB/s eta 0:00:00\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "     ?????????????????????????????????????? 240.6/240.6 kB 32.6 MB/s eta 0:00:00\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "     ?????????????????????????????????????? 196.4/196.4 kB 30.1 MB/s eta 0:00:00\n",
      "Collecting tzlocal<6,>=1.1\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting pillow<11,>=7.1.0\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "     ???????????????????????????????????????? 4.5/4.5 MB 76.1 MB/s eta 0:00:00\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "     ???????????????????????????????????????? 4.8/4.8 MB 93.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions<5,>=4.3.0\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "     ?????????????????????????????????????? 996.9/996.9 kB 60.3 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata<8,>=1.4\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Collecting tornado<7,>=6.0.3\n",
      "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "     ?????????????????????????????????????? 435.4/435.4 kB 43.9 MB/s eta 0:00:00\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "     ???????????????????????????????????????? 97.9/97.9 kB 17.6 MB/s eta 0:00:00\n",
      "Collecting packaging<24,>=16.8\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ???????????????????????????????????????? 53.0/53.0 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting toml<2,>=0.10.1\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting pyarrow>=6.0\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "     ???????????????????????????????????????? 38.3/38.3 MB 41.1 MB/s eta 0:00:00\n",
      "Collecting tenacity<9,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting requests<3,>=2.27\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ???????????????????????????????????????? 62.6/62.6 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting validators<1,>=0.2\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting python-dateutil<3,>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ?????????????????????????????????????? 247.7/247.7 kB 33.8 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "     ?????????????????????????????????????? 346.6/346.6 kB 36.4 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "     ?????????????????????????????????????? 133.2/133.2 kB 21.3 MB/s eta 0:00:00\n",
      "Collecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "     ???????????????????????????????????????? 85.5/85.5 kB 14.3 MB/s eta 0:00:00\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "     ???????????????????????????????????????? 56.1/56.1 kB 9.2 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ???????????????????????????????????????? 62.7/62.7 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "     ?????????????????????????????????????? 104.6/104.6 kB 17.6 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "     ???????????????????????????????????????? 61.6/61.6 kB 10.8 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "     ?????????????????????????????????????? 142.1/142.1 kB 17.2 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "     ?????????????????????????????????????? 162.5/162.5 kB 17.8 MB/s eta 0:00:00\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "     ???????????????????????????????????????? 1.2/1.2 MB 66.2 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ???????????????????????????????????????? 87.5/87.5 kB 14.2 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.32.1-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     ???????????????????????????????????????? 1.2/1.2 MB 70.6 MB/s eta 0:00:00\n",
      "Collecting attrs>=22.2.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "     ???????????????????????????????????????? 60.8/60.8 kB 11.2 MB/s eta 0:00:00\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, zipp, watchdog, validators, urllib3, tzlocal, tzdata, typing-extensions, tornado, toolz, toml, tenacity, smmap, six, rpds-py, pygments, protobuf, pillow, packaging, numpy, mdurl, MarkupSafe, idna, click, charset-normalizer, certifi, cachetools, blinker, attrs, requests, referencing, python-dateutil, pyarrow, plotly, markdown-it-py, jinja2, importlib-metadata, gunicorn, gitdb, rich, pydeck, pandas, jsonschema-specifications, gitpython, jsonschema, altair, streamlit, streamlit-autorefresh\n",
      "Successfully installed MarkupSafe-2.1.4 altair-5.2.0 attrs-23.2.0 blinker-1.7.0 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 gitdb-4.0.11 gitpython-3.1.41 gunicorn-21.2.0 idna-3.6 importlib-metadata-7.0.1 jinja2-3.1.3 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 markdown-it-py-3.0.0 mdurl-0.1.2 numpy-1.26.3 packaging-23.2 pandas-2.2.0 pillow-10.2.0 plotly-5.18.0 protobuf-4.25.2 pyarrow-15.0.0 pydeck-0.8.1b0 pygments-2.17.2 python-dateutil-2.8.2 pytz-2023.3.post1 referencing-0.32.1 requests-2.31.0 rich-13.7.0 rpds-py-0.17.1 six-1.16.0 smmap-5.0.1 streamlit-1.30.0 streamlit-autorefresh-1.0.1 tenacity-8.2.3 toml-0.10.2 toolz-0.12.1 tornado-6.4 typing-extensions-4.9.0 tzdata-2023.4 tzlocal-5.2 urllib3-2.1.0 validators-0.22.0 watchdog-3.0.0 zipp-3.17.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.3.2\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container eae6be3c4c3d\n",
      " ---> f972f8d60809\n",
      "Step 6/6 : ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8080\", \"--server.address=0.0.0.0\"]\n",
      " ---> Running in 241324e1bffb\n",
      "Removing intermediate container 241324e1bffb\n",
      " ---> 9310f2fd3e5b\n",
      "Successfully built 9310f2fd3e5b\n",
      "Successfully tagged us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-app-web-4-template-autorefresh-streamlit/4-template-autofresh-stremalit:latest\n",
      "PUSH\n",
      "Pushing us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-app-web-4-template-autorefresh-streamlit/4-template-autofresh-stremalit\n",
      "The push refers to repository [us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-app-web-4-template-autorefresh-streamlit/4-template-autofresh-stremalit]\n",
      "a1048b6e92d5: Preparing\n",
      "3304d7b71d2e: Preparing\n",
      "538366cb37e7: Preparing\n",
      "8d8c83534788: Preparing\n",
      "208f3039ddd6: Preparing\n",
      "6aec8a143ee5: Preparing\n",
      "f757a77eef71: Preparing\n",
      "a876dfc51caa: Preparing\n",
      "5bb1de08f5af: Preparing\n",
      "0dfa23fffa41: Preparing\n",
      "aa904f36746c: Preparing\n",
      "6aec8a143ee5: Waiting\n",
      "f757a77eef71: Waiting\n",
      "a876dfc51caa: Waiting\n",
      "5bb1de08f5af: Waiting\n",
      "0dfa23fffa41: Waiting\n",
      "aa904f36746c: Waiting\n",
      "208f3039ddd6: Layer already exists\n",
      "8d8c83534788: Layer already exists\n",
      "6aec8a143ee5: Layer already exists\n",
      "f757a77eef71: Layer already exists\n",
      "a876dfc51caa: Layer already exists\n",
      "5bb1de08f5af: Layer already exists\n",
      "0dfa23fffa41: Layer already exists\n",
      "aa904f36746c: Layer already exists\n",
      "3304d7b71d2e: Pushed\n",
      "538366cb37e7: Pushed\n",
      "a1048b6e92d5: Pushed\n",
      "latest: digest: sha256:a91495d41721ea59a59059f0c6da1ffc270eb2fc6c0ed9368e7e8962c81f3183 size: 2636\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                                                                                                     STATUS\n",
      "a05b31fc-0b27-4809-ae3c-b4b19757eef1  2024-01-26T14:49:18+00:00  2M8S      gs://cmpc-innovation-cd4ml-test_cloudbuild/source/1706280565.768663-3b2dba8108bd46918e895b93933d830a.tgz  us-east1-docker.pkg.dev/cmpc-innovation-cd4ml-test/repo-app-web-4-template-autorefresh-streamlit/4-template-autofresh-stremalit (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "##### VERY IMPORTANT NOTATION\n",
    "\n",
    "# NOTE: The variable names in the gcloud command correspond to the variables defined in the configuration file\n",
    "#yaml\n",
    "\n",
    "# NOTE2: to pass the name of the variables (as always) you must use the dollar sign \"$\" but you must\n",
    "# to be enclosed in quotes (so that it is understood that it is the variable to be replaced in the configuration yaml)\n",
    "\n",
    "# NOTE3: it must be double quotes and without spaces to avoid problems\n",
    "\n",
    "! gcloud builds submit \\\n",
    "    --config=cloudbuild.yaml \\\n",
    "    --substitutions=_LOCATION=\"$REGION\",_REPOSITORY=\"$NAME_REPO\",_IMAGE=\"$NAME_IMAGE\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a446b9-1751-43fd-a0f0-82befe5b6c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28fba7d5",
   "metadata": {},
   "source": [
    "### Step 5: Deploy the artifact registry container image to cloud run\r\n",
    "You must run the gloud run deploy command (same as in the container registry) with the only difference being that it changes the location of the image, which is:\r\n",
    "\r\n",
    "  {LOCATION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{IMAGE}/\r\n",
    " \r\n",
    " \r\n",
    "\r\n",
    "**IMPORTANT: DUE TO PERMISSIONS ISSUES, THE CLOUD RUN IS CONFIGURED SO THAT ANYONE WITH THE LINK CAN ACCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51740ac6-8482-439b-bd23-0ce4a81edc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### como setear variables de ambiente en cloud run\n",
    "#--set-env-vars=PROJECT_GCP=$PROJECT_ID \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f4016e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deploying container to Cloud Run service [app-web-4-template-autorefesh-streamlit] in project [cmpc-innovation-cd4ml-test] region [us-east1]\n",
      "Deploying...\n",
      "Setting IAM Policy.........................done\n",
      "Creating Revision................done\n",
      "Routing traffic..........................................................................................................................................................................................................................................................................................................................................done\n",
      "Done.\n",
      "Service [app-web-4-template-autorefesh-streamlit] revision [app-web-4-template-autorefesh-streamlit-00004-qik] has been deployed and is serving 100 percent of traffic.\n",
      "Service URL: https://app-web-4-template-autorefesh-streamlit-z33tzzez7a-ue.a.run.app\n"
     ]
    }
   ],
   "source": [
    "! gcloud run deploy $NAME_CLOUD_RUN \\\n",
    "    --image $REGION-docker.pkg.dev/$PROJECT_ID/$NAME_REPO/$NAME_IMAGE \\\n",
    "    --region $REGION \\\n",
    "    --set-env-vars=PROJECT_GCP=$PROJECT_ID \\\n",
    "    --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4000f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
